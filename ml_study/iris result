import numpy as np
from sklearn.svm import SVC, LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, KFold,StratifiedKFold
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.preprocessing import MaxAbsScaler, RobustScaler

#1. 데이터
datasets = load_iris()
x = datasets['data']
y = datasets.target

x_train, x_test, y_train, y_test =train_test_split(x,y,train_size=0.8, shuffle=True, random_state=42) # train사이즈는 kfold가 발리데이션이ㄴ;까 포함한 0.8

#kfold train,test 셋 나눌 때 아무데나 넣어도 상관없음. kfold 선언이라서 
n_splits = 5 # 11이면 홀수 단위로 자름 10개가 train, 1개가 test
random_state = 42
kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state) 

# sclaer 적용
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

#2. 모델
from xgboost import XGBClassifier
model = XGBClassifier(colsample_bylevel=0, colsample_bynode=0.1, colsample_bytree=1,gamma=4,
                      learning_rate=0.1, max_depth=3, min_child_weight=1,n_estimators=100)

#3. 훈련 
import time
start_time = time.time()
model.fit(x_train,y_train)
end_time = time.time() - start_time

#4. 평가 예측
score = cross_val_score(model, 
                        x_train, y_train, 
                        cv=kfold) #cv: cross validation
print("cross validation acc: ",score)
#cv acc 는 kfold의 n splits 만큼 값이 나옴 . 

y_predict = cross_val_predict(model, 
                              x_test, y_test,
                              cv=kfold)
print('cv predict : ', y_predict)
#0.2 y_test 만큼 개수 나옴: cv predict 

acc = accuracy_score(y_test, y_predict)
print('cv predict acc : ', acc)
# cv predict acc 
# cv predict acc :  0.9666666666666667